<script setup>
</script>

<template>
	<!-- Special Sponsored Meeting -->
	<div class="section pt-0">
		<h3 class="text-3xl font-bold flex flex-row gap-3 mb-5">
			News: Special Sponsored Meeting on AI and Regulation at WCCI 2024
		</h3>
		<p>
			This year, apart from our Special Session on ‘AI, Law and Regulation’,
			we are pleased to announce that we will be hosting a
			<b>Special INNS Sponsored Meeting on AI and Regulation at WCCI 2024</b>.
		</p>
		<p>
			This year’s Tuesday INNS discussion evening will facilitate an open, collaborative discussion on the 
			nexus of AI and Regulations. It will be an opportunity to hear from researchers, legal academics, and
			industry representatives on the current and imminent AI regulatory landscape and how it will impact 
			how we design, develop and deploy Neural Networks models.
		</p>
		<p>
			Approaching a complex spectrum of intersecting challenges requires interdisciplinary approaches and 
			multi-stakeholder participation. At this special meeting, we will be able to ask and attempt to address 
			pertinent questions when pursuing trustworthy and responsible AI. Namely, how should we navigate the 
			complex regulatory landscape? How do we envisage the future of responsible AI governance from the onset?
		</p>
		<h3 class="font-bold md:mt-5">
			Find out more about AI and Regulation Evening Discussion
		</h3>
		<p>
			In response to the global regulatory issues and movements, we are pleased to announce INNS’s evening
			discussion theme of ‘AI and Regulations’, organized for the first time in the history of IJCNN/WCCI.
		</p>
		<p>
			We will hear from five panellists representing leading academics in IT, Technology and IP Law, Lawyers 
			and Data and AI Ethicist, followed by a Q&A. It will provide insight into the legal frameworks currently 
			being developed and implemented on AI globally, the foreseeable changes in this domain, and how they will 
			impact the work of our AI pioneers.
		</p>
		<p>
			We will hear from:
		</p>
		<ul class="space-y-5">
			<li>
				<div class="flex flex-col">
					<u>Mateja Durovic</u>
					<i class="text-sm">
						Professor of Law at King's College London, Deputy Director of the Law School's Centre for Technology,
						Ethics, Law and Society (TELOS) and Academic Director of the Executive programme on Consumer Law
					</i>
					<b class="pt-2 font-semibold">The Brussels (D)Effect: First-Mover Disadvantages in the AI Regulation</b>
					<p>
						The ongoing Fourth Industrial Revolution and the omnipresent digitalization phenomenon have brought many
						new benefits to the market, society, economy, citizens, but they have also posed a number of novel
						regulatory challenges. It may be observed that the contemporary digital world may represent a significant
						disruption to the existing regulation in the most diverse spheres of a legal system.  In that context, the
						main regulatory issues are related to the questions when and how to properly regulate new technologies.
						As a result, the task of providing an adequate regulatory response to the digitalization has led to a very
						competitive race among diverse jurisdictions on a global scale on how to approach these challenges.
					</p>
				</div>
			</li>
			
			<li>
				<div class="flex flex-col">
					<u>Colton Crum</u>
					<i class="text-sm">Graduate Assistant at University of Notre Dame, US</i>
					<u>Cary Coglianese</u>
					<i class="text-sm">Edward B. Shils Professor of Law Director, Penn Program on Regulation University of Pennsylvania</i>
					<b class="pt-2 font-semibold">Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence</b>
					<p>
						Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are
						leading to the adoption around the world of what regulatory scholars have called a management-based approach
						to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory
						standards by the International Organization for Standardization, share in common a core management-based paradigm.
						These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and
						developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within
						this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate
						some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better
						addressing the needs for fairness and effective explainability. In this paper, we discuss the connection between
						the emerging management based regulatory frameworks governing AI and the need for human oversight during training.
						We broadly cover some of the technical components involved in human-guided training and then argue that the kinds
						of high-stakes use cases for AI that appear of most concern to regulators should lean more on human-guided training
						than on data-only training. We hope to foster a discussion between legal scholars and computer scientists involving
						how to govern a domain of technology that is vast, heterogenous, and dynamic in its applications and risks.
					</p>
				</div>
			</li>
			
			<li>
				<div class="flex flex-col">
					<u>Nicola Fabiano:</u>
					<i class="text-sm">Lawyer at Studio Legale Fabiano, Adjunct Professor, Ostrava University (Rome)</i>
					<b class="pt-2 font-semibold">
						AI Act and Large Language Models (LLMs): When Critical Issues and Privacy Impact require Human and 
						Ethical Oversight
					</b>
					<p>
						The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes
						it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy,
						personal data protection, and at an ethical level, especially on the weakest and most vulnerable. The contribution
						addresses human oversight, ethical oversight, and privacy impact assessment.
					</p>
				</div>
			</li>
			
			<li>
				<div class="flex flex-col">
					<u>Nicole Inverardi</u>
					<i class="text-sm">Intesa Sanpaolo S.p.A. Chief Data, A.I., Innovation and Technology Officer, Data and AI Ethicist</i>
					<b class="pt-2 font-semibold">Generative AI and its impacts on fundamental rights: the crucial role of explainability</b>
					<p>
						Generative AI holds immense potential, but its impacts on fundamental rights requires analysis. One of the
						ethical principles that becomes more challenging to be addressed with generative AI is explainability and
						the related “right to explanation”. That happens primarily because of their black box nature, probabilistic
						generation of outputs with elements of randomness involved, and the continuous learning and adaptiveness.
						Open questions remain for responsible development and deployment to safeguard fundamental rights.
					</p>
				</div>
			</li>
			
			<li>
				<div class="flex flex-col">
					<u>Maja Nisevic</u>
					<i class="text-sm">Postdoctoral Researcher at KU Leuven at the Centre for IT & IP Law</i>
					<b class="pt-2 font-semibold">Civil Liability and AI Systems in Medicine</b>
					<p>
						AI systems in medicine, often perceived as integrated machines within hospital systems designed to assist
						humans, introduce a complex' one actor in-chain effect' in civil liability. This implies that each actor in
						the chain could potentially be held liable, depending on the various interactions and collaborations within
						the chain.
					</p>
					<p>
						Suppose the chain starts with the development of AI systems (i.e., from AI developer to the whole automated
						system supported by ML and Big Data) and goes further over the producer, the user who ordered the AI system,
						the end user who provides treatment, and patient in the end. In that case, the question is whether it is
						ethical to put the stress of civil liability on the patient by stressing the importance of the consent form.
						Is a consent form enough to reduce moral responsibility, or is it ethical to use it to reduce the legal liability
						of the doctors or creator of an AI system in a particular case? On another page, the question is whether
						it is ethical to put the stress of civil liability on a medical practitioner who is employing such a system.
						Can we say a doctor is liable by emphasizing the importance of malpractice even though vulnerabilities can also
						treat doctor knowledge regarding AI in general?
					</p>
					<p>
						The legal framework in the EU regarding a potential relevant liability regime is under the test concerning a
						range of different legal acts relevant to the medical sector specifically. Therefore, it is questionable how
						to decide between specific sectorial and non-specific sectorial rules when it comes to the type of harm
						potentially possible to be provoked by AI systems malfunctions or the use of AI systems in medicine.
					</p>
				</div>
			</li>
			
		</ul>
		<div class="flex gap-3">
			<div class="flex flex-col">
				<b>Organisers:</b>
				<b>Moderator:</b>				
			</div>
			<div class="flex flex-col">
				Asim Roy, Amanda Horzyk
				<div class="flex flex-col">
					Amanda Horzyk
					<i class="text-sm">Researcher in Innovation, Technology and the Law, University of Edinburgh.</i>
					<a class="text-sm" href="mailto:amandahorzyk@outlook.com">amandahorzyk@outlook.com</a>
				</div>
			</div>
		</div>
	</div>
</template>

<style scoped></style>